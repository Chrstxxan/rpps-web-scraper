"""
esse é o script principal que integra todas as etapas do projeto:
- busca links das atas
- baixa os arquivos
- extrai os metadados (tipo, data, etc)
- gera relatórios em jsonl e txt organizados
"""

import argparse
import json
from pathlib import Path
from core.discovery import find_meeting_links
from core.downloader import download_files
from core.extractor import extract_metadata_from_files
from core.metadata import save_metadata
from core.utils import setup_directories

# lista de sites base (pags iniciais dos municipios)
RPPS_SITES = [
    {"name": "IPMO Osasco", "uf": "SP", "url": "https://www.ipmosasco.com.br/"},
    {"name": "IPSMI Itaquaquecetuba", "uf": "SP", "url": "https://ipsmi.itaquaquecetuba.sp.gov.br/"},
    {"name": "FUPREVAS Vassouras", "uf": "RJ", "url": "https://www.vassouras.rj.gov.br/category/fuprevas/"},
    {"name": "ToledoPrev", "uf": "PR", "url": "https://toledoprev.toledo.pr.gov.br/"},
    {"name": "FPGPREV Praia Grande", "uf": "SP", "url": "https://www2.praiagrande.sp.gov.br/"},
    # Extra opcional para validar robustez e adaptabilidade
    {"name": "FPMU Umuarama", "uf": "PR", "url": "https://fpmu.umuarama.pr.gov.br/"},
]

def parse_args():
    # permite rodar o script com parâmetro --out
    parser = argparse.ArgumentParser(description="Coleta Autônoma de Atas de RPPS")
    parser.add_argument("--out", default="./data", help="Diretório base de saída (default: ./data)")
    return parser.parse_args()


def main():
    # funcao principal que roda o fluxo completo
    args = parse_args()
    base_out = Path(args.out)
    base_out.mkdir(parents=True, exist_ok=True)

    print("Iniciando busca de atas de RPPS...\n")

    all_metadata = []  # guarda todos os metadados consolidados

    for site in RPPS_SITES:
        # adiciona a tag "(Extra)" se o nome do RPPS contiver "umuarama"
        tag = " (Extra)" if "Umuarama" in site["name"] else ""
        print(f"Buscando atas em: {site['name']} ({site['uf']}){tag}")

        # cria o diretorio organizado para o RPPS atual
        base_path = setup_directories(site["name"], site["uf"], base_out)

        # descobre links que contenham atas
        links = find_meeting_links(site["url"])

        if not links:
            print(f"Nenhuma ata encontrada para {site['name']}. Pulando...\n")
            continue

        # faz o download dos arquivos
        downloaded_files = download_files(links, base_path, site)

        # extrai metadados e analisa tipo e data das reuniões
        metadata = extract_metadata_from_files(downloaded_files, site)

        # acumula todos os metadados pra gerar o relatorio geral no fim
        all_metadata.extend(metadata)

        # define onde salvar os relatorios individuais
        output_dir = base_path / "relatorios"
        output_dir.mkdir(exist_ok=True)

        # salva relatorios em JSONL e TXT (por RPPS)
        save_metadata(metadata, output_dir)

        print(f"Concluído: {site['name']} ({len(downloaded_files)} arquivos)\n")

    # cria os relatorios consolidados (geral de todos os RPPS)
    merged_jsonl = base_out / "atas_geral.jsonl"
    merged_txt = base_out / "atas_geral.txt"

    try:
        # salva o JSONL consolidado
        with open(merged_jsonl, "w", encoding="utf-8") as jf:
            for entry in all_metadata:
                jf.write(json.dumps(entry, ensure_ascii=False) + "\n")

        # salva o TXT consolidado
        with open(merged_txt, "w", encoding="utf-8") as tf:
            tf.write("Relatório geral consolidado de todas as atas coletadas\n")
            tf.write("=" * 80 + "\n\n")
            for entry in all_metadata:
                tf.write(f"RPPS: {entry.get('rpps')} ({entry.get('uf')})\n")
                tf.write(f"Tipo de Reunião: {entry.get('tipo_reuniao')}\n")
                tf.write(f"Data: {entry.get('data_reuniao')}\n")
                tf.write(f"Arquivo: {entry.get('file_name')}\n")
                tf.write(f"Origem: {entry.get('source_page')}\n")
                tf.write(f"Link: {entry.get('file_url')}\n")
                tf.write("-" * 80 + "\n")

        print(f"Relatórios gerais salvos em:\n - {merged_jsonl}\n - {merged_txt}")

    except Exception as e:
        print(f"Erro ao salvar relatório consolidado: {e}")

    print("Processo finalizado com sucesso!")

if __name__ == "__main__":
    main()
